Jupyter Server: local
'Python Interactive': Idle



[1]



import os
import numpy as np
import pandas as pd
[2]


df = pd.read_csv("coast.csv")
df.head(5)
Type	Network Name	connection	nodes	edges	average shortest path(of subgraphs)	over-all average clusteringdegree	over-all median clusteringdegree	35% average clusteringdegree	35% median clusteringdegree	50% average clusteringdegree	50% median clusteringdegree	core number over medium	core_number average	Number of triangles	diameter	number of Communities	number of nodes in the largest community	Unnamed: 18	location
0	undirected	1bingham82Rel	unconnected	10004	362894	2.477634	0.222889	0.183824	0.375593	0.318991	0.324402	0.273008	44.0	37.896441	7163862	NaN	9	1906	NaN	west coast
1	undirected	2bowdoin47Rel	unconnected	2252	84387	2.223140	0.288719	0.268838	0.410020	0.364202	0.372620	0.333333	49.0	41.308615	2131455	NaN	7	698	NaN	east coast
2	undirected	3brandeis99Rel	unconnected	3898	137567	1.981479	0.262103	0.235294	0.401298	0.343668	0.356770	0.307692	43.0	37.714726	3056655	NaN	12	1077	NaN	east coast
3	undirected	4brown11Rel	unconnected	8600	384526	2.394555	0.217382	0.183333	0.351249	0.290967	0.305745	0.252714	56.0	46.908023	8858001	NaN	15	1856	NaN	west coast
4	undirected	5bucknell39Rel	unconnected	3826	158864	2.139104	0.278460	0.248184	0.413032	0.359623	0.369063	0.322638	51.0	44.236539	3981768	NaN	7	883	NaN	east coast
[3]


columns = df.columns.tolist()
data_train = df[[columns[i] for i in range(2, 18) if i != 15]].copy()
data_label = df[columns[-1]].copy()
[4]


data_train.head(5)
connection	nodes	edges	average shortest path(of subgraphs)	over-all average clusteringdegree	over-all median clusteringdegree	35% average clusteringdegree	35% median clusteringdegree	50% average clusteringdegree	50% median clusteringdegree	core number over medium	core_number average	Number of triangles	number of Communities	number of nodes in the largest community
0	unconnected	10004	362894	2.477634	0.222889	0.183824	0.375593	0.318991	0.324402	0.273008	44.0	37.896441	7163862	9	1906
1	unconnected	2252	84387	2.223140	0.288719	0.268838	0.410020	0.364202	0.372620	0.333333	49.0	41.308615	2131455	7	698
2	unconnected	3898	137567	1.981479	0.262103	0.235294	0.401298	0.343668	0.356770	0.307692	43.0	37.714726	3056655	12	1077
3	unconnected	8600	384526	2.394555	0.217382	0.183333	0.351249	0.290967	0.305745	0.252714	56.0	46.908023	8858001	15	1856
4	unconnected	3826	158864	2.139104	0.278460	0.248184	0.413032	0.359623	0.369063	0.322638	51.0	44.236539	3981768	7	883
[5]


data_label.head(5)
0    west coast
1    east coast
2    east coast
3    west coast
4    east coast
Name: location, dtype: object
[6]


data_train_processed = data_train.replace("unconnected", 0)
data_train_processed = data_train_processed.replace("disconnected", 0)
data_train_processed = data_train_processed.replace("connected", 1)
[7]


data_train_processed.head(5)
connection	nodes	edges	average shortest path(of subgraphs)	over-all average clusteringdegree	over-all median clusteringdegree	35% average clusteringdegree	35% median clusteringdegree	50% average clusteringdegree	50% median clusteringdegree	core number over medium	core_number average	Number of triangles	number of Communities	number of nodes in the largest community
0	0	10004	362894	2.477634	0.222889	0.183824	0.375593	0.318991	0.324402	0.273008	44.0	37.896441	7163862	9	1906
1	0	2252	84387	2.223140	0.288719	0.268838	0.410020	0.364202	0.372620	0.333333	49.0	41.308615	2131455	7	698
2	0	3898	137567	1.981479	0.262103	0.235294	0.401298	0.343668	0.356770	0.307692	43.0	37.714726	3056655	12	1077
3	0	8600	384526	2.394555	0.217382	0.183333	0.351249	0.290967	0.305745	0.252714	56.0	46.908023	8858001	15	1856
4	0	3826	158864	2.139104	0.278460	0.248184	0.413032	0.359623	0.369063	0.322638	51.0	44.236539	3981768	7	883
[8]


data_label_processed = data_label.replace("west coast", 1)
data_label_processed = data_label_processed.replace("east coast", 2)
data_label_processed = data_label_processed.replace("middle", 0)
[9]


data_label_processed.head(5)
0    1
1    2
2    2
3    1
4    2
Name: location, dtype: int64
[10]


from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score
[11]


from imblearn.over_sampling import SMOTE
[12]


total_scores = []
X_data = data_train_processed.to_numpy()
y_data = data_label_processed.to_numpy()
for _ in range(7):
    rnd = np.random.permutation(len(y_data))
    X = X_data[rnd]
    y = y_data[rnd]
    total_scores.append(cross_val_score(KNeighborsClassifier(), X, y, cv=10))
total_scores
[array([0.57142857, 0.71428571, 0.57142857, 0.71428571, 0.71428571,
        0.57142857, 0.57142857, 0.71428571, 0.71428571, 0.83333333]),
 array([0.71428571, 0.57142857, 0.57142857, 0.71428571, 0.57142857,
        0.71428571, 0.85714286, 0.57142857, 0.85714286, 0.66666667]),
 array([0.71428571, 0.57142857, 0.71428571, 0.71428571, 0.57142857,
        0.71428571, 0.57142857, 0.71428571, 0.71428571, 0.66666667]),
 array([0.42857143, 0.71428571, 0.85714286, 0.14285714, 0.71428571,
        0.71428571, 0.71428571, 0.57142857, 0.71428571, 0.83333333]),
 array([0.71428571, 0.71428571, 0.71428571, 0.71428571, 0.57142857,
        0.57142857, 0.85714286, 0.71428571, 0.57142857, 0.5       ]),
 array([0.71428571, 0.71428571, 0.42857143, 0.71428571, 0.71428571,
        0.85714286, 0.57142857, 0.71428571, 0.71428571, 0.66666667]),
 array([0.57142857, 0.71428571, 0.71428571, 0.71428571, 0.57142857,
        0.57142857, 0.71428571, 0.57142857, 0.42857143, 0.83333333])]
[13]


[x.mean() for x in total_scores]
[0.669047619047619,
 0.6809523809523809,
 0.6666666666666667,
 0.6404761904761905,
 0.6642857142857143,
 0.680952380952381,
 0.6404761904761904]
[14]


a = [x.mean() for x in total_scores]
sum(a) / len(a)
0.6632653061224489
[18]


total_scores = []
X_data = data_train_processed.to_numpy()
y_data = data_label_processed.to_numpy()
sm = SMOTE(k_neighbors=2)
X_data_res, y_data_res = sm.fit_resample(X_data, y_data)
for _ in range(7):
    rnd = np.random.permutation(len(y_data_res))
    X = X_data_res[rnd]
    y = y_data_res[rnd]
    total_scores.append(cross_val_score(KNeighborsClassifier(), X, y, cv=10))

[array([0.6       , 0.8       , 0.73333333, 0.86666667, 0.6       ,
        0.8       , 0.86666667, 0.85714286, 0.78571429, 0.78571429]),
 array([0.66666667, 0.73333333, 0.86666667, 0.66666667, 0.86666667,
        0.53333333, 0.86666667, 0.71428571, 0.64285714, 0.85714286]),
 array([0.6       , 0.66666667, 0.53333333, 0.73333333, 0.8       ,
        0.86666667, 0.8       , 0.78571429, 0.78571429, 0.78571429]),
 array([0.8       , 0.8       , 0.8       , 0.66666667, 0.6       ,
        0.66666667, 0.66666667, 0.78571429, 0.71428571, 0.71428571]),
 array([0.66666667, 0.93333333, 0.73333333, 0.93333333, 0.66666667,
        0.6       , 0.6       , 0.78571429, 0.78571429, 0.85714286]),
 array([0.66666667, 0.8       , 0.8       , 0.8       , 0.73333333,
        0.53333333, 0.66666667, 0.71428571, 0.78571429, 0.64285714]),
 array([0.66666667, 0.66666667, 0.6       , 0.6       , 0.8       ,
        0.86666667, 0.73333333, 0.64285714, 0.85714286, 0.92857143])]
[19]


a = [x.mean() for x in total_scores]
sum(a) / len(a)
0.7392517006802721
[20]


from imblearn.over_sampling import KMeansSMOTE, ADASYN, BorderlineSMOTE
total_scores = []
X_data = data_train_processed.to_numpy()
y_data = data_label_processed.to_numpy()
sm = KMeansSMOTE(k_neighbors=2)
X_data_res, y_data_res = sm.fit_resample(X_data, y_data)
for _ in range(7):
    rnd = np.random.permutation(len(y_data_res))
    X = X_data_res[rnd]
    y = y_data_res[rnd]

0.8310204081632653
[23]


total_scores = []
X_data = data_train_processed.to_numpy()
y_data = data_label_processed.to_numpy()
sm = ADASYN(n_neighbors=2)
X_data_res, y_data_res = sm.fit_resample(X_data, y_data)
for _ in range(7):
    rnd = np.random.permutation(len(y_data_res))
    X = X_data_res[rnd]
    y = y_data_res[rnd]
    total_scores.append(cross_val_score(KNeighborsClassifier(), X, y, cv=10))

0.7638095238095238


[24]



total_scores = []
X_data = data_train_processed.to_numpy()
y_data = data_label_processed.to_numpy()
sm = BorderlineSMOTE(k_neighbors=2)
X_data_res, y_data_res = sm.fit_resample(X_data, y_data)
for _ in range(7):
    rnd = np.random.permutation(len(y_data_res))
    X = X_data_res[rnd]
    y = y_data_res[rnd]
    total_scores.append(cross_val_score(KNeighborsClassifier(), X, y, cv=10))

0.7941496598639457

